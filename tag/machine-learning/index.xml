<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning | Christian Kragh Jespersen</title>
    <link>https://astrockragh.github.io/tag/machine-learning/</link>
      <atom:link href="https://astrockragh.github.io/tag/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Machine Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>Â© Christian Kragh Jespersen 2022</copyright><lastBuildDate>Mon, 19 Sep 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://astrockragh.github.io/media/logo_hu99cbd5be059c86a221a329805c1c065d_47879_300x300_fit_lanczos_3.png</url>
      <title>Machine Learning</title>
      <link>https://astrockragh.github.io/tag/machine-learning/</link>
    </image>
    
    <item>
      <title>Building better galaxies with Mangrove</title>
      <link>https://astrockragh.github.io/post/mangrove/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://astrockragh.github.io/post/mangrove/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Mangrove&amp;amp;rsquo;s Logo&#34; srcset=&#34;
               /post/mangrove/featured_old_hu7a91daf6b1a6feed761a05f58cec4d84_279687_0ed301d449071dd9ae425fdc4e9d0a3f.webp 400w,
               /post/mangrove/featured_old_hu7a91daf6b1a6feed761a05f58cec4d84_279687_1278bdb7b01b11e886e8712c7da4ae56.webp 760w,
               /post/mangrove/featured_old_hu7a91daf6b1a6feed761a05f58cec4d84_279687_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://astrockragh.github.io/post/mangrove/featured_old_hu7a91daf6b1a6feed761a05f58cec4d84_279687_0ed301d449071dd9ae425fdc4e9d0a3f.webp&#34;
               width=&#34;760&#34;
               height=&#34;480&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;First of all, I&amp;rsquo;ve written two versions of this post, this one is a little easier to understand (less jargon) and more fun! The other version can be found under &amp;ldquo;Research&amp;rdquo;.&lt;/p&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;When scientists discover something, there are three main things that we tend to ask ourselves.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What is the building blocks of that thing?&lt;/li&gt;
&lt;li&gt;What is keeping the building blocks together in that particular structure?&lt;/li&gt;
&lt;li&gt;How did the thing get assembled?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;One can think of it sort of like getting a new thing from IKEA. We discover a new object, say a galaxy, and it feels like we have discovered a new item in the IKEA catalogue that is our world.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Boxes from IKEA&#34; srcset=&#34;
               /post/mangrove/ikea_boxes_hua16e68f23b3733410713f9c2fa435ae2_928645_128d4430dd2e499ed4064fbe0abb1cb5.webp 400w,
               /post/mangrove/ikea_boxes_hua16e68f23b3733410713f9c2fa435ae2_928645_da2fa61c82a22b2929637bb1391a5e4b.webp 760w,
               /post/mangrove/ikea_boxes_hua16e68f23b3733410713f9c2fa435ae2_928645_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://astrockragh.github.io/post/mangrove/ikea_boxes_hua16e68f23b3733410713f9c2fa435ae2_928645_128d4430dd2e499ed4064fbe0abb1cb5.webp&#34;
               width=&#34;760&#34;
               height=&#34;623&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;But now we want to investigate it a bit more. We want to answer our three questions, and for that we just need to open the box and find the assembly manual, within which we find:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What parts is included for building our galaxy, i.e. the building blocks. For galaxies this would be some stars, some dark matter, some gas and so on.&lt;/li&gt;
&lt;li&gt;What tools are included to build our galaxy. For our galaxy we mainly just need one screwdriver, gravity.&lt;/li&gt;
&lt;li&gt;The instructions for how to use our tools to put together our parts.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Question 1 is actually not as simple as it looks. When we go out and look at galaxies, we see that there seems to a building block that affects the galaxy a lot, but that we cannot see. This is what we call &lt;strong&gt;dark matter&lt;/strong&gt;. While dark matter is weird, it is also quite simple. As far as we can tell, it only interacts with other things through gravity, but it is still fundamental to how galaxies end up being. This is because it collapses under its own gravity and makes gravitational wells where galaxies can form. These collapsed clumps are called halos.&lt;/p&gt;
&lt;p&gt;Question 3 is by far the most complicated when it comes to galaxies, because it is really hard to observe. Galaxies evolve over v e r y long timescales.
Luckily, we have simulations we can investigate this with. There are a couple of key problems with the simulations though.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;They are computationally expensive.&lt;/li&gt;
&lt;li&gt;They are hard interpret/understand.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Making a model that emulates a simulation can solve problem 1 and give a lot of new insights into problem 2. So that is what we have done by building &lt;em&gt;Mangrove&lt;/em&gt;. Mangrove is a Machine Learning based emulator that takes into account all three core aspects of galaxy formations, building blocks, tools and assembly manual, to give us better galaxies faster.&lt;/p&gt;
&lt;h2 id=&#34;technical-stuff&#34;&gt;Technical stuff&lt;/h2&gt;
&lt;p&gt;First of all, we don&amp;rsquo;t completely bypass the simulation step, since we still need to run a simulation, but only one with &lt;em&gt;nothing but dark matter&lt;/em&gt;. Because dark matter is so simple, this can be done pretty quickly. Then, once we have that simulation, we can try to &amp;ldquo;paint&amp;rdquo; galaxies on top of the dark matter. This is by no means the first time this has been attempted, but earlier papers have only included features from the final time step of the simulation as their input to their model. This essentially means that these methods can learn answers to questions 1 and 2 (current building blocks/tools), but are agnostic about the assembly of the galaxies. Mangrove improves on this by using not just the final time step, but an encoding of the temporal evolution of the dark matter known as a &lt;strong&gt;merger tree&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The merger tree is made by simply stopping the simulation at different time steps and identifying the halos where we know that galaxies live. We then see which halos merge with other halos from timestep to timestep, and build a tree-like structure encoding this evolution (see Fig. 1).&lt;/p&gt;
&lt;!-- ![]() --&gt;
&lt;figure&gt;
&lt;img src=&#34;graphic1.png&#34; alt=&#34;First graphic&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.1 - Running a simulation and identifying halos at different time steps allows us to build the merger tree, which can then easily be encoded as a graph. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We can then encode the merger tree as a mathematical &lt;strong&gt;graph&lt;/strong&gt;, which in this context simply means a collection of nodes and edges, where each node represents a halo, and the edges determine how they have interacted with each other over time. When we have the graphs we can use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Graph_neural_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graph Neural Network (GNN)&lt;/a&gt;, to learn how the galaxies pertaining to those merger trees should be.&lt;/p&gt;
&lt;p&gt;In order to build an emulator, we also have to find something to emulate. In this post I discuss results from emulating the outputs from a Semi-Analytic Model&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; but it works just as well for other kinds of simulations, like magneto-hydrodynamical simulations like &lt;a href=&#34;https://www.tng-project.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IllustrisTNG&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Mangrove is a Graph Neural Network (GNN). GNNs work by acting on both nodes, and the &lt;strong&gt;neighbourhood&lt;/strong&gt; of that node, meaning all the nodes that is connected to it by an edge.
The way we learn from both nodes and neighbourhoods is through &lt;em&gt;Message Passing&lt;/em&gt;. Message passing means that each node sends information about its current state along the edges of the graph, and is then updated by a learnable&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; function &lt;em&gt;f&lt;/em&gt;. This information passed along the edges is then used to update the state of the node through another learnable&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; function &lt;em&gt;g&lt;/em&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;graphic2.png&#34; alt=&#34;Second graphic&#34; style=&#34;width:100%&#34;&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;img src=&#34;graphic3.png&#34; alt=&#34;Second graphic&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.2 - Messages are passed along edges, and the nodes are updated. The updated nodes are then summed over and decoded to give us the properties of the galaxy that resulted from that merger tree. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;After doing a couple of these message passing steps, we then sum over all halos in the merger tree and use &lt;strong&gt;yet another learnable function&lt;/strong&gt; &lt;em&gt;h&lt;/em&gt;, that decodes this sum and predicts the aspects of galaxies that we are interested in, such as the total mass of the stars in the galaxy, the mass of the black hole at the center and the amount of cold gas, as well as the uncertainties on these quantities.&lt;/p&gt;
&lt;p&gt;We like having uncertainties on our predictions, since we recognize that there might be a little area around the true value that would also be reasonable to predict. Furthermore, it gives the model the possiblity of simply letting us know if it doesn&amp;rsquo;t know what&amp;rsquo;s going on, instead of making a foolhardy guess.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;How much better does Mangrove do compared to methods you ask?&lt;/p&gt;
&lt;p&gt;Well, let&amp;rsquo;s compare how well assign stellar masses to halos work with different methods!&lt;/p&gt;
&lt;p&gt;The first comparison is with the traditional method for assigning galaxy properties based on their dark matter halos, Abundance Matching. Abundance Matching is quite simple, assuming only that there exists a monotonic relationship between halo and galaxy properties. That means that if we were to assign stellar masses to the halos, the most massive halo would get the highest stellar mass, the least massive the least stellar mass and so on. If we compare the abundance matched relationship to the one we predict, we see a dramatic difference.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;precision_abundance_match.png&#34; alt=&#34;precision graphic one&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.3 - We show the relationship between what we are supposed to be getting on the x-axis and what we actually predict using either method on the y-axis. A perfect prediction would mean that all point were on the diagonal, therefore the goal is to cluster as tightly around the diagonal as possible. The difference is quite apparent. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;But, that is perhaps an unfair comparison. We use machine learning after all, and abundance matching uses nothing but a single number per halo.&lt;/p&gt;
&lt;p&gt;We should then maybe compare to other machine learning methods that use a lot of information &lt;em&gt;but&lt;/em&gt; only from the final halo. That seems more fair.&lt;/p&gt;
&lt;p&gt;But as is apparent, Mangrove still outperforms this by about a factor of 2 when measured by how much the two methods deviate from perfect predictions.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;precision_halo.png&#34; alt=&#34;precision graphic two&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.4 - Same as Fig. 3, but comparing to the state of the art machine learning techniques. The difference is still striking. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Mangrove can also predict a lot of other things, like the amount of cold gas in a galaxy (M&lt;sub&gt;cold&lt;/sub&gt;), how fast it is making stars right now (Star Formation Rate/SFR), how much metal there is in the gas (Z&lt;sub&gt;gas&lt;/sub&gt;) and the mass of the black hole at its center (M&lt;sub&gt;BH&lt;/sub&gt;).&lt;/p&gt;
&lt;p&gt;Mangrove does better than all of the above mentioned methods across the board.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;precision_others.png&#34; alt=&#34;precision graphic three&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.5 - Same as Fig. 4/5, but here we show how other galaxy properties, all improvements over the current state of the art. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;other-fun-things&#34;&gt;Other fun things&lt;/h3&gt;
&lt;p&gt;Now that we have a well-working model, we can then try to figure out where the improvement comes from. Is it in the first few timesteps, the initial conditions? Can we make galaxies at different spots in time?&lt;/p&gt;
&lt;p&gt;The answer to the last question turns out to be yes. If we take a single Mangrove model and train it at several different times (we encode this as a &lt;a href=&#34;https://en.wikipedia.org/wiki/Redshift&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;redshift&lt;/a&gt;, usually written as a &lt;strong&gt;z&lt;/strong&gt;), we see that it both does well at the new times, but also in between!&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;interpolation.png&#34; alt=&#34;interpolation&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.6 - Mangrove can do galaxies across time, and interpolate to timesteps it has never seen before! &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In terms of removing things from the model, we tried to simply cut down the merger tree from above, leaving out the very earliest halos. We found that the closer you get to the present day, the more each halo matters. The effect is so strong that removing the first half of the merger tree doesn&amp;rsquo;t matter much, but removing the final 1% is measurable!&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;trimming.png&#34; alt=&#34;interpolation&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.7 - By removing parts of the merger tree, we see that most of the information comes from times closer to the present day. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;If there is one thing that one should remember from this paper, it is that the long-standing &amp;rsquo;nature versus nurture&amp;rsquo; debate is not just for humans. It also matters for galaxies, and with our code, Mangrove, we can take both aspects into account, and make big improvements!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It matters &lt;em&gt;how&lt;/em&gt; we build our galaxies!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;p&gt;Anyone who wishes to do anything similar to this project is encouraged &lt;a href=&#34;https://ui.adsabs.harvard.edu/search/p_=0&amp;amp;q=author%3A%22Jespersen%2C%20Christian%20K.%22&amp;amp;sort=date%20desc%2C%20bibcode%20desc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;read the paper&lt;/a&gt; and check out the &lt;a href=&#34;https://github.com/astrockragh/Mangrove&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;. All the code used for the projects is free to use, although I do not guarantee that it will be easy to find your way around in it!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Semi Analytic Models are a kind of simulation that is run after running a simulation of only the dark matter in the universe. A good early reference is &lt;a href=&#34;https://arxiv.org/pdf/astro-ph/9802268.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/astro-ph/9802268.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Any time I mention a &amp;ldquo;learnable function&amp;rdquo;, think about it as a very basic neural network, what we call a Multi-Layer Perceptron (MLP). An illustration of an MLP can be found at &lt;a href=&#34;https://en.wikipedia.org/wiki/Neural_network#/media/File:Neural_network_example.svg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/Neural_network#/media/File:Neural_network_example.svg&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Learning Galaxy Properties from Merger Trees with Mangrove</title>
      <link>https://astrockragh.github.io/project/mangrove/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://astrockragh.github.io/project/mangrove/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Mangrove&amp;amp;rsquo;s Logo&#34; srcset=&#34;
               /project/mangrove/featured_old_hu7a91daf6b1a6feed761a05f58cec4d84_279687_0ed301d449071dd9ae425fdc4e9d0a3f.webp 400w,
               /project/mangrove/featured_old_hu7a91daf6b1a6feed761a05f58cec4d84_279687_1278bdb7b01b11e886e8712c7da4ae56.webp 760w,
               /project/mangrove/featured_old_hu7a91daf6b1a6feed761a05f58cec4d84_279687_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://astrockragh.github.io/project/mangrove/featured_old_hu7a91daf6b1a6feed761a05f58cec4d84_279687_0ed301d449071dd9ae425fdc4e9d0a3f.webp&#34;
               width=&#34;760&#34;
               height=&#34;480&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;First of all, I&amp;rsquo;ve written two versions of this post, this one is a little harder to understand (more jargon) but more concise! The other version can be found under &amp;ldquo;Blog posts&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Efficiently mapping baryonic properties onto dark matter is a major challenge in astrophysics.
Although semi-analytic models (SAMs)&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and hydrodynamical simulations have made impressive advances in reproducing galaxy observables across cosmologically significant volumes, these methods still require significant computation times, representing a barrier to many applications.&lt;/p&gt;
&lt;p&gt;Some advances have been made thanks to Machine Learning models, which learn mappings between the much less expensive dark matter only simulations and baryonic galaxy properties. However, these ML emulators still lack a lot of precision, and provide only few insights into the processes that they emulate! This is mainly because they try to map directly between dark matter halos and galaxies, ignoring formation and environmental effects.&lt;/p&gt;
&lt;p&gt;In this project, we improved on both the precision and interpretability of previous work in the field, by including the formation history and showing that when it comes to nature versus nurture for galaxies, &lt;strong&gt;nurture matters a lot&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;We do so by introducing &lt;strong&gt;Mangrove&lt;/strong&gt;, a Graph Neural Network which works on &lt;strong&gt;merger trees&lt;/strong&gt;, which encode the formation history of the dark matter halo.&lt;/p&gt;
&lt;p&gt;Mangrove is twice as precise as other ML models, and allows us to put galaxies into dark matter simulations between 10&lt;sup&gt;4&lt;/sup&gt; and 10&lt;sup&gt;9&lt;/sup&gt; times faster than a SAM and IllustrisTNG codes, respectively.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Illustris&#34; srcset=&#34;
               /project/mangrove/speed_hueba3f1d752cddd088ef83ef6f6b329ab_671846_93f34f88f82f45456778f127f323df8a.webp 400w,
               /project/mangrove/speed_hueba3f1d752cddd088ef83ef6f6b329ab_671846_bca160bd2a61d5c41f010841e0e6c868.webp 760w,
               /project/mangrove/speed_hueba3f1d752cddd088ef83ef6f6b329ab_671846_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://astrockragh.github.io/project/mangrove/speed_hueba3f1d752cddd088ef83ef6f6b329ab_671846_93f34f88f82f45456778f127f323df8a.webp&#34;
               width=&#34;681&#34;
               height=&#34;414&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;technical-stuff&#34;&gt;Technical stuff&lt;/h2&gt;
&lt;p&gt;First of all, we don&amp;rsquo;t completely bypass the simulation step, since we still need to run a simulation, but only one with &lt;em&gt;nothing but dark matter&lt;/em&gt;. Because dark matter is so simple, this can be done pretty quickly. Then, once we have that simulation, we can try to &amp;ldquo;paint&amp;rdquo; galaxies on top of the dark matter. This is by no means the first time this has been attempted, but earlier papers have only included features from the final time halo of the simulation as input to their models (a list of relevant references can be found in our paper).&lt;/p&gt;
&lt;p&gt;Mangrove improves on these works by using not just the final halo, but an encoding of the temporal evolution of the dark matter in the shape of a &lt;strong&gt;merger tree&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The merger tree is made by simply stopping the simulation at different times/redshifts and identifying the halos where we know that galaxies live. We then see which halos merge with other halos from timestep to timestep, and build a tree-like structure encoding this evolution (see Fig. 1).&lt;/p&gt;
&lt;!-- ![]() --&gt;
&lt;figure&gt;
&lt;img src=&#34;graphic1.png&#34; alt=&#34;First graphic&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.1 - Running a simulation and identifying halos at different time steps allows us to build the merger tree, which can then easily be encoded as a graph. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We can then encode the merger tree as a mathematical &lt;strong&gt;graph&lt;/strong&gt;, which in this context simply means a collection of nodes and edges, where each node represents a halo, and the edges determine how they have interacted with each other over time. When we have the graphs we can use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Graph_neural_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graph Neural Network (GNN)&lt;/a&gt;, to learn how the galaxies pertaining to those merger trees should be.&lt;/p&gt;
&lt;p&gt;In order to build an emulator, we also have to find something to emulate. In this post I discuss results from emulating the outputs from a Semi-Analytic Model&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; but it works just as well for other kinds of simulations, like magneto-hydrodynamical simulations like &lt;a href=&#34;https://www.tng-project.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IllustrisTNG&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Mangrove is a Graph Neural Network (GNN). GNNs work by acting on both nodes, and the &lt;strong&gt;neighbourhood&lt;/strong&gt; of that node, meaning all the nodes that is connected to it by an edge.
The way we learn from both nodes and neighbourhoods is through &lt;em&gt;Message Passing&lt;/em&gt;. Message passing means that each node sends information about its current state along the edges of the graph, and is then updated by a learnable&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; function &lt;em&gt;f&lt;/em&gt;. This information passed along the edges is then used to update the state of the node through another learnable&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; function &lt;em&gt;g&lt;/em&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;graphic2.png&#34; alt=&#34;Second graphic&#34; style=&#34;width:100%&#34;&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;img src=&#34;graphic3.png&#34; alt=&#34;Second graphic&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.2 - Messages are passed along edges, and the nodes are updated. The updated nodes are then summed over and decoded to give us the properties of the galaxy that resulted from that merger tree. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;After doing a couple of these message passing steps, we then sum over all halos in the merger tree and use &lt;strong&gt;yet another learnable function&lt;/strong&gt; &lt;em&gt;h&lt;/em&gt;, that decodes this sum and predicts the aspects of galaxies that we are interested in, such as the total mass of the stars in the galaxy, the mass of the black hole at the center and the amount of cold gas, as well as the uncertainties on these quantities.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;How much better does Mangrove do compared to methods you ask?&lt;/p&gt;
&lt;p&gt;Well, let&amp;rsquo;s compare how well assign stellar masses to halos work with different methods!&lt;/p&gt;
&lt;p&gt;The first comparison is with the traditional method for assigning galaxy properties based on their dark matter halos, Abundance Matching. Abundance Matching is quite simple, assuming only that there exists a monotonic relationship between halo and galaxy properties. That means that if we were to assign stellar masses to the halos, the most massive halo would get the highest stellar mass, the least massive the least stellar mass and so on. If we compare the abundance matched relationship to the one we predict, we see a dramatic difference. Note that all values are given in logarithmic units.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;precision_abundance_match.png&#34; alt=&#34;precision graphic one&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.3 - We show the relationship between what we are supposed to be getting on the x-axis and what we actually predict using either method on the y-axis. A perfect prediction would mean that all point were on the diagonal, therefore the goal is to cluster as tightly around the diagonal as possible. The difference is quite apparent. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;But, that is perhaps an unfair comparison. We use ML after all, and abundance matching uses nothing but a single number per halo.&lt;/p&gt;
&lt;p&gt;We should then maybe compare to other ML methods that use a lot of information &lt;em&gt;but&lt;/em&gt; only from the final halo. That seems more fair.&lt;/p&gt;
&lt;p&gt;But as is apparent, Mangrove still outperforms this by about a factor of 2 when measured by how much the two methods deviate from perfect predictions.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;precision_halo.png&#34; alt=&#34;precision graphic two&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.4 - Same as Fig. 3, but comparing to the state of the art machine learning techniques. The difference is still striking. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Mangrove can also predict a lot of other things, like the amount of cold gas in a galaxy (M&lt;sub&gt;cold&lt;/sub&gt;), how fast it is making stars right now (Star Formation Rate/SFR), how much metal there is in the gas (Z&lt;sub&gt;gas&lt;/sub&gt;) and the mass of the black hole at its center (M&lt;sub&gt;BH&lt;/sub&gt;).&lt;/p&gt;
&lt;p&gt;Mangrove does better than all of the above mentioned methods across the board, which again, is a simple statement that &lt;strong&gt;formation history matters&lt;/strong&gt;!&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;precision_others.png&#34; alt=&#34;precision graphic three&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.5 - Same as Fig. 4/5, but here we show how other galaxy properties, all improvements over the current state of the art. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;other-fun-things&#34;&gt;Other fun things&lt;/h3&gt;
&lt;p&gt;Now that we have a well-working model, we can then try to figure out where the improvement comes from. Is it in the first few timesteps, the initial conditions? Can we make galaxies at different spots in time?&lt;/p&gt;
&lt;p&gt;The answer to the last question turns out to be yes. If we take a single Mangrove model and train it at several redshifts, we see that it both does well at the new times, but also when interpolating! It is also noteworthy that the model does not become worse when doing the more complicated task of learning a more general mapping that applies at a wider range of redshifts.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;interpolation.png&#34; alt=&#34;interpolation&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.6 - Mangrove can do galaxies across time, and interpolate to redshifts it has never seen before! &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In terms of removing things from the model, we tried to simply cut down the merger tree from above, leaving out the very earliest halos. We found that the closer you get to the present day, the more each halo matters. The effect is so strong that removing the first half of the merger tree doesn&amp;rsquo;t matter much, but removing the final 1% is measurable!&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;trimming.png&#34; alt=&#34;interpolation&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.7 - By removing parts of the merger tree, we see that most of the information comes from times closer to the present day. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;If there is one thing that one should remember from this paper, it is that the long-standing &amp;rsquo;nature versus nurture&amp;rsquo; debate is not just for humans. It also matters for galaxies, and with our code, Mangrove, we can take both aspects into account, and make big improvements!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It matters &lt;em&gt;how&lt;/em&gt; we build our galaxies!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;p&gt;Anyone who wishes to do anything similar to this project is encouraged &lt;a href=&#34;https://ui.adsabs.harvard.edu/search/p_=0&amp;amp;q=author%3A%22Jespersen%2C%20Christian%20K.%22&amp;amp;sort=date%20desc%2C%20bibcode%20desc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;read the paper&lt;/a&gt; and check out the &lt;a href=&#34;https://github.com/astrockragh/Mangrove&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;. All the code used for the projects is free to use, although I do not guarantee that it will be easy to find your way around in it!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Semi Analytic Models are a kind of simulation that is run after running a simulation of only the dark matter in the universe. A good early reference is &lt;a href=&#34;https://arxiv.org/pdf/astro-ph/9802268.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/astro-ph/9802268.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Any time I mention a &amp;ldquo;learnable function&amp;rdquo;, think about it as a very basic neural network, what we call a Multi-Layer Perceptron (MLP). An illustration of an MLP can be found at &lt;a href=&#34;https://en.wikipedia.org/wiki/Neural_network#/media/File:Neural_network_example.svg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/Neural_network#/media/File:Neural_network_example.svg&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Classifying  $V^0$ - particles from the ATLAS detector for detector calibration</title>
      <link>https://astrockragh.github.io/project/puk_troels/</link>
      <pubDate>Tue, 25 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://astrockragh.github.io/project/puk_troels/</guid>
      <description>&lt;p&gt;This project was done in collaboration with &lt;a href=&#34;https://github.com/Vinther901&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonas Vinther&lt;/a&gt;, &lt;a href=&#34;https://github.com/JohannSeverin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Johann Bock Severin&lt;/a&gt; and &lt;a href=&#34;https://github.com/JakobSchauser&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jakob H. Schauser&lt;/a&gt; during our third semester as physics undergraduates. Check out their GitHubs!&lt;/p&gt;
&lt;p&gt;In all branches of physics, new discoveries are made by identifying some signal peak upon a background of noise. In High Energy Physics, this could be the peak of a new particle found amongst the almost uncountable amount of collisions happening every second the Large Hadron Collider is running. Only a few collisions actually produce interesting particles. This makes it crucial to be able to classify which detections are part of the actual signal, and which are not. In this project we employed a range of statistical methods and machine learning algorithms to get the best estimate of the masses of different $V^0$ - particles, in both simulated data and real data.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://astrockragh.github.io/project/puk_troels/Truesimplefits.png&#34; alt=&#34;Signal in simulation and data for normal fit. The background is fitted with a third degree polynomial and a double Gaussian is used on the peak.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;As can be seen in the top figure, among some of the methods tried were simply doing some linear cuts in the feature space, found by simple annealing, using Fisher&amp;rsquo;s discriminant analysis, a simple decision tree and some boosted decision tree algorithms, of which XGBoost ended up being the method of choice. We ended up getting an accuracy of 99.7% for classyfing signal and background.&lt;/p&gt;
&lt;p&gt;Perhaps the most exciting thing from this project was the ability for our models to learn what a true label looked like in real data without having those labels. Overall, the LHC dataset is fantastic as a learning platform for both classification and regression problem. If you would like to get involved, reach out to &lt;a href=&#34;https://www.nbi.dk/~petersen/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Troels C. Petersen&lt;/a&gt;. Anybody who wants to read the resulting report is welcome to do, by clicking the link at the top of the article, where we also explore a range of more or less wacky ideas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Classifying Gamma-Ray Bursts into Two Classes using t-SNE</title>
      <link>https://astrockragh.github.io/project/tsne_grb/</link>
      <pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://astrockragh.github.io/project/tsne_grb/</guid>
      <description>&lt;p&gt;The duration of a gamma-ray burst (GRB) is a key indicator of its physical origin, with long bursts perhaps associated with the collapse of massive stars (also known as supernovae) and short bursts with mergers of neutron stars (kilonovae).
However, trying to categorize these events from the burst itself is very troublesome, as there is substantial overlap in the properties of both short and long GRBs and neither duration nor any other parameter so far considered completely separates the two groups [insert figure]. Some very long bursts even show up without any observble supernova, even when we know exactly where it came from, which even led people to propose extra classes of bursts, like the Extended Short Gamma Ray Burst, to explain the lack of a supernova. In this project, which was done by myself and my two good friends &lt;a href=&#34;https://github.com/JohannSeverin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Johann Bock Severin&lt;/a&gt; and &lt;a href=&#34;https://github.com/Vinther901&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonas Vinther&lt;/a&gt; as a semester project during our first year at the University of Copenhagen, we unambiguously classified every known GRB from the Swift Space observatory, using a dimensionality reduction algorithm, t-distributed stochastic neighborhood embedding, which had never been before and so was quite exciting. Although the classification takes place only using prompt emission light curves, every burst with an associated supernova is found in the longer group and bursts with kilonovae in the short, suggesting along with the duration distributions that these two groups are truly long and short GRBs, or some sort of proxy.&lt;/p&gt;
&lt;p&gt;Since t-SNE works by comparing vectors dimension by dimension, it was crucial that each dimension actually meant the same for each burst. Unfortunately, there is a great deal of uncertainty surrounding exactly when a burst begins or ends, so the lightcurves where rarely aligned such that the first second of a given burst meant the same as the first second of any other burst. The trick behind getting this to work was shifting the bursts from the time domain into the frequency domain using the &amp;lsquo;Fourier Transform&amp;rsquo;, which eliminates temporal translational differences in the lightcurves and makes it possible to actually compare the bursts dimension by dimension!&lt;/p&gt;
&lt;p&gt;Running the machine learning algorithm on the Fourier transformed light curves let&amp;rsquo;s us get these two beautiful groups! TSNE GROUP FIGURE&lt;/p&gt;
&lt;p&gt;One of the interesting things that came out of this project was to determine that those supernovae that we couldn&amp;rsquo;t find before go firmly in the group with all the other supernovae, so we&amp;rsquo;re pretty sure that they actually exploded, but the exact mechanism that led to an explosion with no visible supernova is still unknown. Professor Darach Watson at DAWN Copenhagen [link] currently has my favourite theory for why this is: Instead of the star exploding and then turning into a black hole, it would BOLD &amp;lsquo;implode&amp;rsquo; and turn directly into a black hole!&lt;/p&gt;
&lt;p&gt;Anyone who wishes to do anything similar to this project is encourage to &lt;a href=&#34;https://ui.adsabs.harvard.edu/abs/2020ApJ...896L..20J/abstract&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;read the paper&lt;/a&gt; and check out the [Github](&lt;a href=&#34;https://github.com/astrockragh/GRB_TSNE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/astrockragh/GRB_TSNE&lt;/a&gt;. Currently, we wish to extend this analysis to other observatories, getting thousands of new bursts included in the analysis!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Improving event reconstrution at IceCube</title>
      <link>https://astrockragh.github.io/project/neutrino_ml/</link>
      <pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://astrockragh.github.io/project/neutrino_ml/</guid>
      <description>&lt;p&gt;This Bachelors Thesis was done in collaboration with &lt;a href=&#34;https://github.com/Vinther901&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonas Vinther&lt;/a&gt;, &lt;a href=&#34;https://github.com/JohannSeverin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Johann Bock Severin&lt;/a&gt; and &lt;a href=&#34;https://github.com/JakobSchauser&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jakob H. Schauser&lt;/a&gt; during our sixth semester as physics undergraduates. Check out their GitHubs!&lt;/p&gt;
&lt;p&gt;This project was concerned with improving the reconstruction of neutrino events at The IceCube Neutrino Observatory, an experiment located at The South Pole, which aims to detect neutrinos and other particles in the ice sheet. Currently the observatory is focused on very very high energy neutrinos (TeV-PeV range) where the entire detector lights up and it is pretty easy to see where it came from, as can be seen in the video below.&lt;/p&gt;
&lt;iframe width=&#34;1280&#34; height=&#34;720&#34; src=&#34;https://www.youtube.com/embed/vTya9hoKsfM&#34; title=&#34;High energy neutrino at IceCube&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;.
&lt;p&gt;However, IceCube actually gets many more events at lower energies (GeV range), so we wanted to help improving the reconstruction at these energies. Here, only a few dots light up, and it is hard to figure out anything about the particle. As you may also be able to see in the video, this is further complicated by the fact that the IceCube detector has a somewhat odd two-layer hexagonal-cylindrical shape, which makes traditional algorithms almost impossible to use. We therefore turned to Graph Neural Networks (GNNs), which are very powerful when learning on geometric data. It&amp;rsquo;s worth noting that grids are also special cases of graphs.&lt;/p&gt;
&lt;p&gt;We found superior performance compared to the algorithm currently used by IceCube, which about a factor of a million increase in speed. The project was so successful that a group of people at the Niels Bohr Institute are currently doing a full-scale project, which you can find on the &lt;a href=&#34;https://github.com/icecube/graphnet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IceCube official Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If anyone has any questions, feel free to steal our code, read our report or reach out to me. The four different group members constructed four different model, varying from a few thousand parameters to multiple million, so this is also a great way of looking into model differences for the same task.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>

<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects | Christian Kragh Jespersen</title>
    <link>https://astrockragh.github.io/project/</link>
      <atom:link href="https://astrockragh.github.io/project/index.xml" rel="self" type="application/rss+xml" />
    <description>Projects</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© Christian Kragh Jespersen 2024</copyright><lastBuildDate>Tue, 01 Oct 2024 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://astrockragh.github.io/media/logo_hu99cbd5be059c86a221a329805c1c065d_47879_300x300_fit_lanczos_3.png</url>
      <title>Projects</title>
      <link>https://astrockragh.github.io/project/</link>
    </image>
    
    <item>
      <title>The most massive galaxy we will find with JWST at high redshift</title>
      <link>https://astrockragh.github.io/project/most_massive_jwst-/</link>
      <pubDate>Tue, 01 Oct 2024 00:00:00 +0000</pubDate>
      <guid>https://astrockragh.github.io/project/most_massive_jwst-/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Clustering, or cosmic variance, greatly impacts the observed distribution of massive galaxies at high redshift!&#34; srcset=&#34;
               /project/most_massive_jwst/demo_hu9e98fe34cccdd82f5fa4dbcef6e68fba_1432983_f9951a38b1833ee2e30f6bb8dd1eecb4.webp 400w,
               /project/most_massive_jwst/demo_hu9e98fe34cccdd82f5fa4dbcef6e68fba_1432983_7e2b67f3f8fefe283e458c3d93e37c27.webp 760w,
               /project/most_massive_jwst/demo_hu9e98fe34cccdd82f5fa4dbcef6e68fba_1432983_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://astrockragh.github.io/project/most_massive_jwst/demo_hu9e98fe34cccdd82f5fa4dbcef6e68fba_1432983_f9951a38b1833ee2e30f6bb8dd1eecb4.webp&#34;
               width=&#34;760&#34;
               height=&#34;592&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Building on &lt;a href=&#34;https://ui.adsabs.harvard.edu/abs/2021ApJ...923....8S/abstract&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous work&lt;/a&gt; where we investigated the impact of cosmic variance on the probability of finding a single, ultra-high redshift galaxy, we now turn to the question of the mass of the most massive object in a given observed field. This was motivated by the &lt;a href=&#34;https://ui.adsabs.harvard.edu/abs/2023ApJ...946L..13F/abstract&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;surprising abundance of massive objects observed with JWST&lt;/a&gt;, which challenged all galaxy formation models and even had some people questioning our cosmological model. However, the first analyses of this potential tension was done without much regard for potential subleties, something we attempted to mitigate in this paper. A major issue with many studies is that they often overlook an important factor: &lt;strong&gt;cosmic variance&lt;/strong&gt;. This refers to the uneven clustering of galaxies across different parts of the sky (see the above figure), which causes the number of galaxies observed in a given area to vary more than you’d expect from simple Poisson statistics. In essence, certain regions may systematically have more or fewer galaxies just because of where you&amp;rsquo;re looking, and this systematic is a &lt;strong&gt;big&lt;/strong&gt; effect in the very early universe.&lt;/p&gt;
&lt;p&gt;Cosmic variance becomes even more pronounced when you look at galaxies that are very massive, at high redshifts, or when observing small areas of the sky — just like in the deep surveys conducted by JWST. These narrow and deep surveys targeting the earliest, most massive galaxies, are especially vulnerable to the effects of cosmic variance.&lt;/p&gt;
&lt;p&gt;In this paper, we developed a model to predict the distribution of mass of the most massive galaxies in given JWST surveys, taking the cosmic variance into account. The model shows that previous methods, which relied on &lt;a href=&#34;https://arxiv.org/abs/2208.10479&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;idealized Extreme Value Statistics&lt;/a&gt;, don’t fully capture the actual distribution. Instead, the new predictions reveal some surprising differences, both in the position and shape of the distribution of the largest galaxies, as can be see. The results were a little counterintuitive at first, but upon further reflection, they offer a more accurate picture of what we should expect to (and do) find in these surveys! This is shown in the below figure, where the yellow contours (including cosmic variance) can clearly be seen to greatly diverge from the green contours (no cosmic variance), at high redshift.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The introduction of cosmic variance greatly impact the probability of finding a single massive galaxy, which makes the results much more realistic.&#34; srcset=&#34;
               /project/most_massive_jwst/um_comparison_hucb2ce46c9fcd92de04b91bd04f14d71e_288393_e18e940bc8265bc1673493d1f0498c21.webp 400w,
               /project/most_massive_jwst/um_comparison_hucb2ce46c9fcd92de04b91bd04f14d71e_288393_20ab7d5fc346626f105630db5a278cd0.webp 760w,
               /project/most_massive_jwst/um_comparison_hucb2ce46c9fcd92de04b91bd04f14d71e_288393_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://astrockragh.github.io/project/most_massive_jwst/um_comparison_hucb2ce46c9fcd92de04b91bd04f14d71e_288393_e18e940bc8265bc1673493d1f0498c21.webp&#34;
               width=&#34;760&#34;
               height=&#34;275&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;To test the models validity, we used simulations from &lt;a href=&#34;https://arxiv.org/abs/1806.07893&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;UniverseMachine&lt;/a&gt; (see the red contours in the figure above), an empirically calibrated tool designed to simulate galaxy formation in the universe. Sure enough, the predicted effects of cosmic variance stood out clearly, showing that it has a major impact on the most massive galaxies we may observe, in exactly the way our simple model predicted!&lt;/p&gt;
&lt;p&gt;Another key finding was that cosmic variance estimators are affected by a &amp;ldquo;variance on the variance&amp;rdquo; in a way that has so far been neglected. The uncertainty in the cosmic variance itself is highly non-Gaussian in the limit where the cosmic variance itself is high, and we both prove it and empirically measure the effect. The proof can be checked out in the paper, but the result is shown in the below figure!&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;The errors on cosmic variance are greater than one would think&#34; srcset=&#34;
               /project/most_massive_jwst/demo_sigma_cv_huac8ebcf2b5218e09eb4c8a6de2244082_224715_44897c087cc0d0a1ed10285f3c2dc5a4.webp 400w,
               /project/most_massive_jwst/demo_sigma_cv_huac8ebcf2b5218e09eb4c8a6de2244082_224715_b8ec5d63d2f5c1f2281584cb483c4960.webp 760w,
               /project/most_massive_jwst/demo_sigma_cv_huac8ebcf2b5218e09eb4c8a6de2244082_224715_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://astrockragh.github.io/project/most_massive_jwst/demo_sigma_cv_huac8ebcf2b5218e09eb4c8a6de2244082_224715_44897c087cc0d0a1ed10285f3c2dc5a4.webp&#34;
               width=&#34;760&#34;
               height=&#34;569&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Ultimately, the study underscores the importance of properly considering cosmic variance when interpreting the results of galaxy surveys, especially when trying to reconcile the existence of extreme galaxies in the early universe with existing galaxy formation models. Without doing so, we may be misinterpreting the true nature of the early universe. The effects for a few sample cases can be seen below.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;results&#34; srcset=&#34;
               /project/most_massive_jwst/first_hu5e0e863fe3d9840b6ef3d5873d5e6071_611032_4633218795801727b1ea4e80e4981d15.webp 400w,
               /project/most_massive_jwst/first_hu5e0e863fe3d9840b6ef3d5873d5e6071_611032_791b00077e1e554dddc3f1be9bbe2dce.webp 760w,
               /project/most_massive_jwst/first_hu5e0e863fe3d9840b6ef3d5873d5e6071_611032_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://astrockragh.github.io/project/most_massive_jwst/first_hu5e0e863fe3d9840b6ef3d5873d5e6071_611032_4633218795801727b1ea4e80e4981d15.webp&#34;
               width=&#34;535&#34;
               height=&#34;760&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The galaxy-environment connection revealed by Graph Neural Networks</title>
      <link>https://astrockragh.github.io/project/gnn_environment/</link>
      <pubDate>Fri, 02 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://astrockragh.github.io/project/gnn_environment/</guid>
      <description>&lt;p&gt;To preface all of this, this work, consisting of two projects, was led by the incomparable &lt;a href=&#34;https://jwuphysics.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John F. Wu&lt;/a&gt; at STScI/JHU, and it was a joy to work with him on this! Now on to the research.
The projects were started during the great &lt;a href=&#34;https://datadrivengalaxyevolution.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;KITP program on Data-Driven Astronomy&lt;/a&gt;, taking my earlier work on the connection between &lt;a href=&#34;https://astrockragh.github.io/project/mangrove/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;galaxies and their formation histories&lt;/a&gt;. A natural follow-up question is whether we can find similar improvements in our modelling of galaxies by exploiting their &lt;strong&gt;environments&lt;/strong&gt; instead of their &lt;strong&gt;formation histories&lt;/strong&gt;. We therefore dive into the intricate relationship between galaxies, dark matter halos, and their large-scale cosmic environments using data from the Illustris TNG300 hydrodynamic simulation. To better understand these connections, we tested two machine learning (ML) models: Explainable Boosting Machines (EBMs) and E(3)-invariant graph neural networks (GNNs). We use GNNs to make sure that we capture the full information in the cosmic web, instead of realying on summary statistics, an approach which we know did not work for analyzing formation histories! The E(3)-invariance, means that the functions we learned are made explicitly invariant to &lt;a href=&#34;https://en.wikipedia.org/wiki/Euclidean_group&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;E&lt;/strong&gt;uclidean transfomations&lt;/a&gt; in &lt;strong&gt;3&lt;/strong&gt;D, meaning all translations, rotations, and reflections in 3D.&lt;/p&gt;
&lt;p&gt;One of the main issues with analyzing environments is that the connections between different galaxies are not given &lt;em&gt;a priori&lt;/em&gt;. We therefore must pick a subjective length scale, within which we consider galaxies &amp;ldquo;connected&amp;rdquo;. We must then tune this &lt;strong&gt;linking length&lt;/strong&gt; (L), to give us the best results, and see what we can learn from this tuning. In the below plot, I show how graph structures that result from changing linking lengths.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Changing the linking length means more connections&#34; srcset=&#34;
               /project/gnn_environment/changing_linking_length_huadf1f8ecb3a9b479f02af79a8867c8ec_925841_825f36b36411bb9e43c98cabfaa114ae.webp 400w,
               /project/gnn_environment/changing_linking_length_huadf1f8ecb3a9b479f02af79a8867c8ec_925841_bc965fb5301a60a2cc29b27d45307001.webp 760w,
               /project/gnn_environment/changing_linking_length_huadf1f8ecb3a9b479f02af79a8867c8ec_925841_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://astrockragh.github.io/project/gnn_environment/changing_linking_length_huadf1f8ecb3a9b479f02af79a8867c8ec_925841_825f36b36411bb9e43c98cabfaa114ae.webp&#34;
               width=&#34;760&#34;
               height=&#34;270&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;We now test our GNNs and our EBMs with a summary statistic that summarizes information on the same length scale as that used by the GNN. The summary statistic we choose is the &lt;em&gt;spherically averaged overdensity&lt;/em&gt;, in simple terms, the total amount of mass within a ball of radius L, centered on each galaxy. In summary we have two model, trained to predict the &lt;strong&gt;stellar masses&lt;/strong&gt; starting from &lt;strong&gt;dark matter&lt;/strong&gt;, the GNN using the dark matter halo, and those around it, and the EBM using the halo and the total amount of mass around it. Differences between the two are therefore only due to treatments in environment. The plot below shows our results as a function of linking length. The y-axis shows the reconstruction &lt;strong&gt;error&lt;/strong&gt;, so lower is better!&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Changing the linking length means better performace&#34; srcset=&#34;
               /project/gnn_environment/error_length_hu174f52277a947edd04d4895efa36d3fe_228913_69763fcf07fe47e617296baec565fdb6.webp 400w,
               /project/gnn_environment/error_length_hu174f52277a947edd04d4895efa36d3fe_228913_c922fd1d4f7f139f7f1094ed44927a00.webp 760w,
               /project/gnn_environment/error_length_hu174f52277a947edd04d4895efa36d3fe_228913_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://astrockragh.github.io/project/gnn_environment/error_length_hu174f52277a947edd04d4895efa36d3fe_228913_69763fcf07fe47e617296baec565fdb6.webp&#34;
               width=&#34;760&#34;
               height=&#34;669&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;First, we see that the GNN (purple) is way better than anything else, including the EBM (blue)! So once again, summary statistics are not the way to go, and GNNs are really powerful! However, there is also interesting structure in the plot. The GNN keeps getting better with each increase of the linking length &lt;strong&gt;at the same rate&lt;/strong&gt; until at 3 Mpc, and then it levels off, but to another fixed rate! There seems to be something special about this length scale, since the behaviour of our model is essentially in one mode below 3 Mpc, and another above!&lt;/p&gt;
&lt;p&gt;If we look at the performance of the EBM, we also see a conspicuous curve with its best performance at a &lt;em&gt;&lt;strong&gt;the same length scale!&lt;/strong&gt;&lt;/em&gt;. The 3 Mpc scale is clearly special!&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;There is a special environment length scale&#34; srcset=&#34;
               /project/gnn_environment/error_length_annotate_huffa39afaf9a987e8a2e1ccb1a0c579ba_398601_339c6607a1150b450dd8d63694b4bdaa.webp 400w,
               /project/gnn_environment/error_length_annotate_huffa39afaf9a987e8a2e1ccb1a0c579ba_398601_2b52248d0e9e1618f75e8d308fa17ebc.webp 760w,
               /project/gnn_environment/error_length_annotate_huffa39afaf9a987e8a2e1ccb1a0c579ba_398601_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://astrockragh.github.io/project/gnn_environment/error_length_annotate_huffa39afaf9a987e8a2e1ccb1a0c579ba_398601_339c6607a1150b450dd8d63694b4bdaa.webp&#34;
               width=&#34;760&#34;
               height=&#34;671&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;However, it&amp;rsquo;s also worth noting that the GNNs were able to pick up on environmental influences from larger scales—up to 10 Mpc—showing that there’s more to the galaxy–halo connection than just nearby density.&lt;/p&gt;
&lt;p&gt;One thing that we found quite amusing while doing this project was that many people who work with the cosmic web (large-scale positioning of galaxies) were very adamant that we would fail if we did not use this special algorithm called DisPerSE, which is supposed to provide an advanced topological description of the cosmic web, instead of just connecting things within a given length. However, we found that these simple spherical density measures were more useful than more complex cosmic web distance features derived from the DisPerSE algorithm (green line in the above plot).&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Disperse is not worth much&#34; srcset=&#34;
               /project/gnn_environment/disperse_meme_hubfde6db3e40b49ac8a6369d761899df8_403749_b45d97d6726aad17b8c4359207a55ad7.webp 400w,
               /project/gnn_environment/disperse_meme_hubfde6db3e40b49ac8a6369d761899df8_403749_b13485bd92caa9bcef1f5293af0c0464.webp 760w,
               /project/gnn_environment/disperse_meme_hubfde6db3e40b49ac8a6369d761899df8_403749_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://astrockragh.github.io/project/gnn_environment/disperse_meme_hubfde6db3e40b49ac8a6369d761899df8_403749_b45d97d6726aad17b8c4359207a55ad7.webp&#34;
               width=&#34;720&#34;
               height=&#34;564&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The highest redshift galaxy we will find with JWST</title>
      <link>https://astrockragh.github.io/project/high_z_jwst/</link>
      <pubDate>Mon, 23 Oct 2023 00:00:00 +0000</pubDate>
      <guid>https://astrockragh.github.io/project/high_z_jwst/</guid>
      <description>&lt;p&gt;One of the primary goals for the upcoming James Webb Space Telescope is to observe the first galaxies. Predictions for planned and proposed surveys have typically focused on average galaxy counts, assuming a random distribution of galaxies across the observed field. The first and most-massive galaxies, however, are expected to be tightly clustered, which can be quantified through an effect known as &lt;strong&gt;cosmic variance&lt;/strong&gt;. We show that cosmic variance is likely to be the dominant contribution to uncertainty for high-redshift mass and luminosity functions, and that median high-redshift and high-mass galaxy counts for planned observations lie significantly below average counts. Several different strategies are considered for improving our understanding of the first galaxies, including adding depth, area, and independent pointings. Adding independent pointings is shown to be the most efficient both for discovering the single highest-redshift galaxy and also for constraining mass and luminosity functions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Learning Galaxy Properties from Merger Trees with Mangrove</title>
      <link>https://astrockragh.github.io/project/mangrove/</link>
      <pubDate>Mon, 19 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://astrockragh.github.io/project/mangrove/</guid>
      <description>&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Mangrove&amp;amp;rsquo;s Logo&#34; srcset=&#34;
               /project/mangrove/featured_old_hu7a91daf6b1a6feed761a05f58cec4d84_279687_0ed301d449071dd9ae425fdc4e9d0a3f.webp 400w,
               /project/mangrove/featured_old_hu7a91daf6b1a6feed761a05f58cec4d84_279687_1278bdb7b01b11e886e8712c7da4ae56.webp 760w,
               /project/mangrove/featured_old_hu7a91daf6b1a6feed761a05f58cec4d84_279687_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://astrockragh.github.io/project/mangrove/featured_old_hu7a91daf6b1a6feed761a05f58cec4d84_279687_0ed301d449071dd9ae425fdc4e9d0a3f.webp&#34;
               width=&#34;760&#34;
               height=&#34;480&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;First of all, I&amp;rsquo;ve written two versions of this post, this one is a little harder to understand (more jargon) but more concise! The other version can be found under &amp;ldquo;Blog posts&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Efficiently mapping baryonic properties onto dark matter is a major challenge in astrophysics.
Although semi-analytic models (SAMs)&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; and hydrodynamical simulations have made impressive advances in reproducing galaxy observables across cosmologically significant volumes, these methods still require significant computation times, representing a barrier to many applications.&lt;/p&gt;
&lt;p&gt;Some advances have been made thanks to Machine Learning models, which learn mappings between the much less expensive dark matter only simulations and baryonic galaxy properties. However, these ML emulators still lack a lot of precision, and provide only few insights into the processes that they emulate! This is mainly because they try to map directly between dark matter halos and galaxies, ignoring formation and environmental effects.&lt;/p&gt;
&lt;p&gt;In this project, we improved on both the precision and interpretability of previous work in the field, by including the formation history and showing that when it comes to nature versus nurture for galaxies, &lt;strong&gt;nurture matters a lot&lt;/strong&gt;!&lt;/p&gt;
&lt;p&gt;We do so by introducing &lt;strong&gt;Mangrove&lt;/strong&gt;, a Graph Neural Network which works on &lt;strong&gt;merger trees&lt;/strong&gt;, which encode the formation history of the dark matter halo.&lt;/p&gt;
&lt;p&gt;Mangrove is twice as precise as other ML models, and allows us to put galaxies into dark matter simulations between 10&lt;sup&gt;4&lt;/sup&gt; and 10&lt;sup&gt;9&lt;/sup&gt; times faster than a SAM and IllustrisTNG codes, respectively.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Illustris&#34; srcset=&#34;
               /project/mangrove/speed_hueba3f1d752cddd088ef83ef6f6b329ab_671846_93f34f88f82f45456778f127f323df8a.webp 400w,
               /project/mangrove/speed_hueba3f1d752cddd088ef83ef6f6b329ab_671846_bca160bd2a61d5c41f010841e0e6c868.webp 760w,
               /project/mangrove/speed_hueba3f1d752cddd088ef83ef6f6b329ab_671846_1200x1200_fit_q100_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://astrockragh.github.io/project/mangrove/speed_hueba3f1d752cddd088ef83ef6f6b329ab_671846_93f34f88f82f45456778f127f323df8a.webp&#34;
               width=&#34;681&#34;
               height=&#34;414&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h2 id=&#34;technical-stuff&#34;&gt;Technical stuff&lt;/h2&gt;
&lt;p&gt;First of all, we don&amp;rsquo;t completely bypass the simulation step, since we still need to run a simulation, but only one with &lt;em&gt;nothing but dark matter&lt;/em&gt;. Because dark matter is so simple, this can be done pretty quickly. Then, once we have that simulation, we can try to &amp;ldquo;paint&amp;rdquo; galaxies on top of the dark matter. This is by no means the first time this has been attempted, but earlier papers have only included features from the final time halo of the simulation as input to their models (a list of relevant references can be found in our paper).&lt;/p&gt;
&lt;p&gt;Mangrove improves on these works by using not just the final halo, but an encoding of the temporal evolution of the dark matter in the shape of a &lt;strong&gt;merger tree&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The merger tree is made by simply stopping the simulation at different times/redshifts and identifying the halos where we know that galaxies live. We then see which halos merge with other halos from timestep to timestep, and build a tree-like structure encoding this evolution (see Fig. 1).&lt;/p&gt;
&lt;!-- ![]() --&gt;
&lt;figure&gt;
&lt;img src=&#34;graphic1.png&#34; alt=&#34;First graphic&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.1 - Running a simulation and identifying halos at different time steps allows us to build the merger tree, which can then easily be encoded as a graph. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;We can then encode the merger tree as a mathematical &lt;strong&gt;graph&lt;/strong&gt;, which in this context simply means a collection of nodes and edges, where each node represents a halo, and the edges determine how they have interacted with each other over time. When we have the graphs we can use a &lt;a href=&#34;https://en.wikipedia.org/wiki/Graph_neural_network&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Graph Neural Network (GNN)&lt;/a&gt;, to learn how the galaxies pertaining to those merger trees should be.&lt;/p&gt;
&lt;p&gt;In order to build an emulator, we also have to find something to emulate. In this post I discuss results from emulating the outputs from a Semi-Analytic Model&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; but it works just as well for other kinds of simulations, like magneto-hydrodynamical simulations like &lt;a href=&#34;https://www.tng-project.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IllustrisTNG&lt;/a&gt;!&lt;/p&gt;
&lt;p&gt;Mangrove is a Graph Neural Network (GNN). GNNs work by acting on both nodes, and the &lt;strong&gt;neighbourhood&lt;/strong&gt; of that node, meaning all the nodes that is connected to it by an edge.
The way we learn from both nodes and neighbourhoods is through &lt;em&gt;Message Passing&lt;/em&gt;. Message passing means that each node sends information about its current state along the edges of the graph, and is then updated by a learnable&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; function &lt;em&gt;f&lt;/em&gt;. This information passed along the edges is then used to update the state of the node through another learnable&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt; function &lt;em&gt;g&lt;/em&gt;.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;graphic2.png&#34; alt=&#34;Second graphic&#34; style=&#34;width:100%&#34;&gt;
&lt;/figure&gt;
&lt;figure&gt;
&lt;img src=&#34;graphic3.png&#34; alt=&#34;Second graphic&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.2 - Messages are passed along edges, and the nodes are updated. The updated nodes are then summed over and decoded to give us the properties of the galaxy that resulted from that merger tree. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;After doing a couple of these message passing steps, we then sum over all halos in the merger tree and use &lt;strong&gt;yet another learnable function&lt;/strong&gt; &lt;em&gt;h&lt;/em&gt;, that decodes this sum and predicts the aspects of galaxies that we are interested in, such as the total mass of the stars in the galaxy, the mass of the black hole at the center and the amount of cold gas, as well as the uncertainties on these quantities.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;How much better does Mangrove do compared to methods you ask?&lt;/p&gt;
&lt;p&gt;Well, let&amp;rsquo;s compare how well assign stellar masses to halos work with different methods!&lt;/p&gt;
&lt;p&gt;The first comparison is with the traditional method for assigning galaxy properties based on their dark matter halos, Abundance Matching. Abundance Matching is quite simple, assuming only that there exists a monotonic relationship between halo and galaxy properties. That means that if we were to assign stellar masses to the halos, the most massive halo would get the highest stellar mass, the least massive the least stellar mass and so on. If we compare the abundance matched relationship to the one we predict, we see a dramatic difference. Note that all values are given in logarithmic units.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;precision_abundance_match.png&#34; alt=&#34;precision graphic one&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.3 - We show the relationship between what we are supposed to be getting on the x-axis and what we actually predict using either method on the y-axis. A perfect prediction would mean that all point were on the diagonal, therefore the goal is to cluster as tightly around the diagonal as possible. The difference is quite apparent. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;But, that is perhaps an unfair comparison. We use ML after all, and abundance matching uses nothing but a single number per halo.&lt;/p&gt;
&lt;p&gt;We should then maybe compare to other ML methods that use a lot of information &lt;em&gt;but&lt;/em&gt; only from the final halo. That seems more fair.&lt;/p&gt;
&lt;p&gt;But as is apparent, Mangrove still outperforms this by about a factor of 2 when measured by how much the two methods deviate from perfect predictions.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;precision_halo.png&#34; alt=&#34;precision graphic two&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.4 - Same as Fig. 3, but comparing to the state of the art machine learning techniques. The difference is still striking. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;Mangrove can also predict a lot of other things, like the amount of cold gas in a galaxy (M&lt;sub&gt;cold&lt;/sub&gt;), how fast it is making stars right now (Star Formation Rate/SFR), how much metal there is in the gas (Z&lt;sub&gt;gas&lt;/sub&gt;) and the mass of the black hole at its center (M&lt;sub&gt;BH&lt;/sub&gt;).&lt;/p&gt;
&lt;p&gt;Mangrove does better than all of the above mentioned methods across the board, which again, is a simple statement that &lt;strong&gt;formation history matters&lt;/strong&gt;!&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;precision_others.png&#34; alt=&#34;precision graphic three&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.5 - Same as Fig. 4/5, but here we show how other galaxy properties, all improvements over the current state of the art. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;other-fun-things&#34;&gt;Other fun things&lt;/h3&gt;
&lt;p&gt;Now that we have a well-working model, we can then try to figure out where the improvement comes from. Is it in the first few timesteps, the initial conditions? Can we make galaxies at different spots in time?&lt;/p&gt;
&lt;p&gt;The answer to the last question turns out to be yes. If we take a single Mangrove model and train it at several redshifts, we see that it both does well at the new times, but also when interpolating! It is also noteworthy that the model does not become worse when doing the more complicated task of learning a more general mapping that applies at a wider range of redshifts.&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;interpolation.png&#34; alt=&#34;interpolation&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.6 - Mangrove can do galaxies across time, and interpolate to redshifts it has never seen before! &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;In terms of removing things from the model, we tried to simply cut down the merger tree from above, leaving out the very earliest halos. We found that the closer you get to the present day, the more each halo matters. The effect is so strong that removing the first half of the merger tree doesn&amp;rsquo;t matter much, but removing the final 1% is measurable!&lt;/p&gt;
&lt;figure&gt;
&lt;img src=&#34;trimming.png&#34; alt=&#34;interpolation&#34; style=&#34;width:100%&#34;&gt;
&lt;figcaption align = &#34;center&#34;&gt;&lt;b&gt;Fig.7 - By removing parts of the merger tree, we see that most of the information comes from times closer to the present day. &lt;/b&gt;&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;If there is one thing that one should remember from this paper, it is that the long-standing &amp;rsquo;nature versus nurture&amp;rsquo; debate is not just for humans. It also matters for galaxies, and with our code, Mangrove, we can take both aspects into account, and make big improvements!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;It matters &lt;em&gt;how&lt;/em&gt; we build our galaxies!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id=&#34;code&#34;&gt;Code&lt;/h2&gt;
&lt;p&gt;Anyone who wishes to do anything similar to this project is encouraged &lt;a href=&#34;https://ui.adsabs.harvard.edu/search/p_=0&amp;amp;q=author%3A%22Jespersen%2C%20Christian%20K.%22&amp;amp;sort=date%20desc%2C%20bibcode%20desc&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;read the paper&lt;/a&gt; and check out the &lt;a href=&#34;https://github.com/astrockragh/Mangrove&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Github&lt;/a&gt;. All the code used for the projects is free to use, although I do not guarantee that it will be easy to find your way around in it!&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Semi Analytic Models are a kind of simulation that is run after running a simulation of only the dark matter in the universe. A good early reference is &lt;a href=&#34;https://arxiv.org/pdf/astro-ph/9802268.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/astro-ph/9802268.pdf&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Any time I mention a &amp;ldquo;learnable function&amp;rdquo;, think about it as a very basic neural network, what we call a Multi-Layer Perceptron (MLP). An illustration of an MLP can be found at &lt;a href=&#34;https://en.wikipedia.org/wiki/Neural_network#/media/File:Neural_network_example.svg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://en.wikipedia.org/wiki/Neural_network#/media/File:Neural_network_example.svg&lt;/a&gt;&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Classifying  $V^0$ - particles from the ATLAS detector for detector calibration</title>
      <link>https://astrockragh.github.io/project/puk_troels/</link>
      <pubDate>Tue, 25 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://astrockragh.github.io/project/puk_troels/</guid>
      <description>&lt;p&gt;This project was done in collaboration with &lt;a href=&#34;https://github.com/Vinther901&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonas Vinther&lt;/a&gt;, &lt;a href=&#34;https://github.com/JohannSeverin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Johann Bock Severin&lt;/a&gt; and &lt;a href=&#34;https://github.com/JakobSchauser&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jakob H. Schauser&lt;/a&gt; during our third semester as physics undergraduates. Check out their GitHubs!&lt;/p&gt;
&lt;p&gt;In all branches of physics, new discoveries are made by identifying some signal peak upon a background of noise. In High Energy Physics, this could be the peak of a new particle found amongst the almost uncountable amount of collisions happening every second the Large Hadron Collider is running. Only a few collisions actually produce interesting particles. This makes it crucial to be able to classify which detections are part of the actual signal, and which are not. In this project we employed a range of statistical methods and machine learning algorithms to get the best estimate of the masses of different $V^0$ - particles, in both simulated data and real data.&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://astrockragh.github.io/project/puk_troels/Truesimplefits.png&#34; alt=&#34;Signal in simulation and data for normal fit. The background is fitted with a third degree polynomial and a double Gaussian is used on the peak.&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;As can be seen in the top figure, among some of the methods tried were simply doing some linear cuts in the feature space, found by simple annealing, using Fisher&amp;rsquo;s discriminant analysis, a simple decision tree and some boosted decision tree algorithms, of which XGBoost ended up being the method of choice. We ended up getting an accuracy of 99.7% for classyfing signal and background.&lt;/p&gt;
&lt;p&gt;Perhaps the most exciting thing from this project was the ability for our models to learn what a true label looked like in real data without having those labels. Overall, the LHC dataset is fantastic as a learning platform for both classification and regression problem. If you would like to get involved, reach out to &lt;a href=&#34;https://www.nbi.dk/~petersen/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Troels C. Petersen&lt;/a&gt;. Anybody who wants to read the resulting report is welcome to do, by clicking the link at the top of the article, where we also explore a range of more or less wacky ideas.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Classifying Gamma-Ray Bursts into Two Classes using t-SNE</title>
      <link>https://astrockragh.github.io/project/tsne_grb/</link>
      <pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://astrockragh.github.io/project/tsne_grb/</guid>
      <description>&lt;p&gt;The duration of a gamma-ray burst (GRB) is a key indicator of its physical origin, with long bursts perhaps associated with the collapse of massive stars (also known as supernovae) and short bursts with mergers of neutron stars (kilonovae).
However, trying to categorize these events from the burst itself is very troublesome, as there is substantial overlap in the properties of both short and long GRBs and neither duration nor any other parameter so far considered completely separates the two groups [insert figure]. Some very long bursts even show up without any observble supernova, even when we know exactly where it came from, which even led people to propose extra classes of bursts, like the Extended Short Gamma Ray Burst, to explain the lack of a supernova. In this project, which was done by myself and my two good friends &lt;a href=&#34;https://github.com/JohannSeverin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Johann Bock Severin&lt;/a&gt; and &lt;a href=&#34;https://github.com/Vinther901&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonas Vinther&lt;/a&gt; as a semester project during our first year at the University of Copenhagen, we unambiguously classified every known GRB from the Swift Space observatory, using a dimensionality reduction algorithm, t-distributed stochastic neighborhood embedding, which had never been before and so was quite exciting. Although the classification takes place only using prompt emission light curves, every burst with an associated supernova is found in the longer group and bursts with kilonovae in the short, suggesting along with the duration distributions that these two groups are truly long and short GRBs, or some sort of proxy.&lt;/p&gt;
&lt;p&gt;Since t-SNE works by comparing vectors dimension by dimension, it was crucial that each dimension actually meant the same for each burst. Unfortunately, there is a great deal of uncertainty surrounding exactly when a burst begins or ends, so the lightcurves where rarely aligned such that the first second of a given burst meant the same as the first second of any other burst. The trick behind getting this to work was shifting the bursts from the time domain into the frequency domain using the &amp;lsquo;Fourier Transform&amp;rsquo;, which eliminates temporal translational differences in the lightcurves and makes it possible to actually compare the bursts dimension by dimension!&lt;/p&gt;
&lt;p&gt;Running the machine learning algorithm on the Fourier transformed light curves let&amp;rsquo;s us get these two beautiful groups! TSNE GROUP FIGURE&lt;/p&gt;
&lt;p&gt;One of the interesting things that came out of this project was to determine that those supernovae that we couldn&amp;rsquo;t find before go firmly in the group with all the other supernovae, so we&amp;rsquo;re pretty sure that they actually exploded, but the exact mechanism that led to an explosion with no visible supernova is still unknown. Professor Darach Watson at DAWN Copenhagen [link] currently has my favourite theory for why this is: Instead of the star exploding and then turning into a black hole, it would BOLD &amp;lsquo;implode&amp;rsquo; and turn directly into a black hole!&lt;/p&gt;
&lt;p&gt;Anyone who wishes to do anything similar to this project is encourage to &lt;a href=&#34;https://ui.adsabs.harvard.edu/abs/2020ApJ...896L..20J/abstract&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;read the paper&lt;/a&gt; and check out the [Github](&lt;a href=&#34;https://github.com/astrockragh/GRB_TSNE&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/astrockragh/GRB_TSNE&lt;/a&gt;. Currently, we wish to extend this analysis to other observatories, getting thousands of new bursts included in the analysis!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Improving event reconstrution at IceCube</title>
      <link>https://astrockragh.github.io/project/neutrino_ml/</link>
      <pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://astrockragh.github.io/project/neutrino_ml/</guid>
      <description>&lt;p&gt;This Bachelors Thesis was done in collaboration with &lt;a href=&#34;https://github.com/Vinther901&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jonas Vinther&lt;/a&gt;, &lt;a href=&#34;https://github.com/JohannSeverin&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Johann Bock Severin&lt;/a&gt; and &lt;a href=&#34;https://github.com/JakobSchauser&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Jakob H. Schauser&lt;/a&gt; during our sixth semester as physics undergraduates. Check out their GitHubs!&lt;/p&gt;
&lt;p&gt;This project was concerned with improving the reconstruction of neutrino events at The IceCube Neutrino Observatory, an experiment located at The South Pole, which aims to detect neutrinos and other particles in the ice sheet. Currently the observatory is focused on very very high energy neutrinos (TeV-PeV range) where the entire detector lights up and it is pretty easy to see where it came from, as can be seen in the video below.&lt;/p&gt;
&lt;iframe width=&#34;1280&#34; height=&#34;720&#34; src=&#34;https://www.youtube.com/embed/vTya9hoKsfM&#34; title=&#34;High energy neutrino at IceCube&#34; frameborder=&#34;0&#34; allow=&#34;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&#34; allowfullscreen&gt;&lt;/iframe&gt;.
&lt;p&gt;However, IceCube actually gets many more events at lower energies (GeV range), so we wanted to help improving the reconstruction at these energies. Here, only a few dots light up, and it is hard to figure out anything about the particle. As you may also be able to see in the video, this is further complicated by the fact that the IceCube detector has a somewhat odd two-layer hexagonal-cylindrical shape, which makes traditional algorithms almost impossible to use. We therefore turned to Graph Neural Networks (GNNs), which are very powerful when learning on geometric data. It&amp;rsquo;s worth noting that grids are also special cases of graphs.&lt;/p&gt;
&lt;p&gt;We found superior performance compared to the algorithm currently used by IceCube, which about a factor of a million increase in speed. The project was so successful that a group of people at the Niels Bohr Institute are currently doing a full-scale project, which you can find on the &lt;a href=&#34;https://github.com/icecube/graphnet&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IceCube official Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If anyone has any questions, feel free to steal our code, read our report or reach out to me. The four different group members constructed four different model, varying from a few thousand parameters to multiple million, so this is also a great way of looking into model differences for the same task.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Point Spread Function improvement for COSMOS2020 survey</title>
      <link>https://astrockragh.github.io/project/cosmos/</link>
      <pubDate>Thu, 20 Jan 2022 00:00:00 +0000</pubDate>
      <guid>https://astrockragh.github.io/project/cosmos/</guid>
      <description>&lt;h2 id=&#34;the-two-catalogs-httpscosmos2020caletorg&#34;&gt;The two CATALOGS: &lt;a href=&#34;https://cosmos2020.calet.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://cosmos2020.calet.org/&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The COSMOS survey is one the longest-running galaxy surveys. Over two decades, we have collected observations of x-ray, optical, infrared, and radio light to obtain galaxy distances and understand their evolution over 13.7 billion years of cosmic history. &lt;a href=&#34;https://astroweaver.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John R. Weaver&lt;/a&gt; at the &lt;a href=&#34;https://cosmicdawn.dk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cosmic Dawn Center&lt;/a&gt; lead the 2020 edition of the master galaxy catalog, using bleeding-edge galaxy model-fitting techniques.&lt;/p&gt;
&lt;p&gt;I worked with John to improve what astronomers call a Point Spread Function, or PSF for short, which essentially details how light becomes spread out and noisy as it moves through the atmosphere. As one might imagine, this isn&amp;rsquo;t the same across the night sky, due to the atmosphere being quite different when looking in different directions. As can be seen in the below figure, if one separates the night-sky into many smaller patches, giving each patch its own PSF, things are a lot better!&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img src=&#34;https://astrockragh.github.io/project/cosmos/psf_homogenization.png&#34; alt=&#34;PSF&#34; loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
